# QuestionAnsweringBert

In this experiment, I explored fine-tuning a large language model (LLM) for question answering. I used Google's BERT model as the base architecture.


For training data, I leveraged the SQuAD dataset, a popular benchmark for question answering tasks.  To optimize training efficiency for this experiment, I utilized a representative subset of the dataset and trained the model for 3 epochs.


The fine-tuned model is currently hosted on Hugging Face, a public platform for sharing and exploring machine learning models. You can access and try it out here: https://huggingface.co/horger89/SlightlyFineTunedBertOnSquad
